
<article>
    <h1>A3 - LPPD-DG Model Selection and Deployment</h1>
    <div class="post-metadata">
        <time datetime="2025-10-09">2025-10-09</time>
        <span class="post-tags">
            Tags: project
        </span>
    </div>
    <h2>Brief</h2>
<p>For the first integrating event we explored the potential deployment and model selection parameters in our design space.<br />
This was done through an initial LAMBDA event and trade off curve generation.</p>
<p>We trained 3 potential models, analyzed these models, then made a final selection based on the lessons learned in the process.</p>
<h2>Current state</h2>
<h3>Memory</h3>
<p>From <code>cargo size</code>:</p>
<pre class="codehilite"><code>pico-2w-doodle-rs  :
section               size        addr
.vector_table          276  0x10000000
.start_block            40  0x10000114
.text               102484  0x1000013c
.bi_entries             12  0x10019190
.rodata             251560  0x100191a0
.data                  168  0x20000000
.gnu.sgstubs             0  0x10056900
.bss                 36624  0x200000a8
.uninit               1024  0x20008fb8
.end_block               0  0x10056900
.defmt                 227         0x0
.comment               139         0x0
.ARM.attributes         56         0x0
Total               392610
</code></pre>

<p>Flash:</p>
<pre class="codehilite"><code>.text       102,484 bytes  (~100 KB)  -  program code
.rodata     251,560 bytes  (~245 KB)  - Constants, strings, WiFi firmware
.other        1,328 bytes  (~1.3 KB)  - Vector table, boot info, etc.
──────────────────────────────────────
Total:      ~354 KB out of 2048 KB (17% used)
Available:  ~1694 KB free ✅
</code></pre>

<p>Ram:</p>
<pre class="codehilite"><code>.data           168 bytes  (~0.2 KB)  - Initialized globals/statics
.bss         36,624 bytes  (~36 KB)   - Uninitialized globals/statics  
.uninit       1,024 bytes  (~1 KB)    - Explicitly uninitialized
──────────────────────────────────────
Total:       ~38 KB out of 512 KB (7.4% used)
Available:   ~474 KB free ✅
</code></pre>

<h3>Latency</h3>
<p><em>no data yet</em></p>
<h2>Choosing a training lib</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Sum Sign</th>
<th style="text-align: left;">Metric</th>
<th>PyTorch</th>
<th>Tensorflow</th>
<th>Onnx</th>
<th>Burn</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Deployment Process</td>
<td>Train -&gt; Convert onnx -&gt; Import in Burn</td>
<td>Train -&gt; Convert onnx  -&gt; Import in Burn</td>
<td>Train -&gt; Import in Burn</td>
<td>Train</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Language</td>
<td>Python</td>
<td>Python</td>
<td>Python</td>
<td>Rust</td>
</tr>
<tr>
<td style="text-align: left;">-</td>
<td style="text-align: left;">Training Development Effort (1 least - 5 most)</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td style="text-align: left;">-</td>
<td style="text-align: left;">Porting Development Effort (1 least -5 most)</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td style="text-align: left;">+</td>
<td style="text-align: left;">Useful Skill Factor (1 least -5 most)</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sums</td>
<td>-1</td>
<td>-2</td>
<td>-3</td>
<td>-3</td>
</tr>
</tbody>
</table>
<h3>Decision</h3>
<p>So we'll train in PyTorch.</p>
<p>The pico 2w supports fp64, fp16 and of course int ops. <br />
We'll design a model with all these for our testing.</p>
<h2>Trained model analysis</h2>
<p>We evaluate the models on a Google Colab CPU instance using ONNX runtime ops.</p>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Size (MB)</th>
<th>CPU Latency (ms)</th>
<th>Works on Pico</th>
<th>CPU Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>4.80</td>
<td>0.7913</td>
<td>No (too large)</td>
<td>99.13</td>
</tr>
<tr>
<td>FP16</td>
<td>2.40</td>
<td>0.6177</td>
<td>No (Unsupported Burn Import Ops)</td>
<td>99.13</td>
</tr>
<tr>
<td>INT8</td>
<td>1.21</td>
<td>0.3923</td>
<td>No (Unsupported Burn Import Ops)</td>
<td>99.09</td>
</tr>
</tbody>
</table>
<p>Lesson learned:<br />
- Pytorch quant models don't easily convert to ONNX. It's easier to quantize through ONNX directly<br />
- <a href="https://burn.dev/books/burn/import/">Burn can actually take a pytorch model</a> so I didn't have to convert to ONNX<br />
- FP32 does not fit on 4MB flash of pico 2w<br />
- FP16  and INT8 seem to have unsupported ops through burn-import</p>
<h2>Trade off curve</h2>
<p>Max: Accuracy<br />
Min: Size, Latency</p>
<figure>
        <img src="/webpage/posts/20251009_a3_lppd_dg_model_selection_and_deployment/Pasted%20image%2020251008214911.png" alt="Pasted image 20251008214911.png" />
    </figure>

<p>INT8 is good candidate. It's smaller, faster, and marginally less accurate than FP options.</p>
<h2>Conclusion</h2>
<p>Although INT8 would be ideal, we're currently forced to deploy on the webapp server and use the FP32 model.<br />
After this project is done I plan to implement <code>QuantizeLinear</code> and <code>DequantizeLinear</code> to burn-import so we can compile the INT8 model.</p>
</article>